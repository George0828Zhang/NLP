{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 4\n",
    "train_name = 'train.csv'\n",
    "test_name = 'test.csv'\n",
    "model_name = 'models/RNNv1'\n",
    "mode = 'chinese' # english / chinese\n",
    "vocab_name = 'vocab.json'\n",
    "sent2seq_name = 'sent2seq.json'\n",
    "sent2seq_test_name = 'sent2seq_test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import jieba.posseg as pseg\n",
    "import csv\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<bos>'\n",
    "EOS = '<eos>'\n",
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "MAX_Q_LEN = 20\n",
    "MAX_A_LEN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocab.json\n",
      "done\n",
      "loading sent2seq.json\n",
      "done\n",
      "Loading train.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c32dc7d8ae4d88bd2784f3464aacd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=320553), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done. 320552 data loaded.\n"
     ]
    }
   ],
   "source": [
    "##### Data Loader\n",
    "print('loading ' + vocab_name)\n",
    "vocab = json.load(open(vocab_name, 'r', encoding='utf-8'))\n",
    "print('done')\n",
    "\n",
    "print('loading ' + sent2seq_name)\n",
    "Sent2Seq = json.load(open(sent2seq_name, 'r'))\n",
    "print('done')\n",
    "\n",
    "iBOS = vocab[BOS]\n",
    "iPAD = vocab[PAD]\n",
    "iEOS = vocab[EOS]\n",
    "\n",
    "print(\"Loading {} ...\".format(train_name))\n",
    "data1 = []\n",
    "data2 = []\n",
    "label = []\n",
    "\n",
    "Reader = csv.reader(open(train_name, newline='', encoding='utf-8'), delimiter=',', quotechar='\"')\n",
    "row_count = sum(1 for row in Reader)\n",
    "Reader = csv.reader(open(train_name, newline='', encoding='utf-8'), delimiter=',', quotechar='\"')\n",
    "for i,fields in tqdm(enumerate(Reader), total=row_count):    \n",
    "    if i == 0:\n",
    "        continue\n",
    "    tid1, tid2 = fields[1:3]\n",
    "#     sent1 = [iBOS] + Sent2Seq[tid1] + [iEOS]\n",
    "#     sent2 = [iBOS] + Sent2Seq[tid2] + [iEOS]\n",
    "    sent1 = Sent2Seq[tid1]\n",
    "    sent2 = Sent2Seq[tid2]\n",
    "    data1.append(sent1)\n",
    "    data2.append(sent2)\n",
    "    label.append(fields[7])\n",
    "NUM_DATA = len(data1)\n",
    "print(\"done. {} data loaded.\".format(NUM_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "data1 = pad_sequences(data1, maxlen=MAX_Q_LEN, padding='pre', truncating='pre', value=iPAD)\n",
    "data2 = pad_sequences(data2, maxlen=MAX_A_LEN, padding='pre', truncating='pre', value=iPAD)\n",
    "print('done')\n",
    "\n",
    "num_agree = sum([lb == 'agreed' for lb in label])\n",
    "num_disagree = sum([lb == 'disagreed' for lb in label])\n",
    "num_unrelated = sum([lb == 'unrelated' for lb in label])\n",
    "\n",
    "lbtype = {'agreed': 0, 'disagreed': 1, 'unrelated': 2}\n",
    "lbweight = {lbtype['agreed']: 1/15, lbtype['disagreed']: 1/5, lbtype['unrelated']: 1/16}\n",
    "# lbweight = {lbtype['agreed']: NUM_DATA/num_agree, lbtype['disagreed']: NUM_DATA/num_disagree, lbtype['unrelated']: NUM_DATA/num_unrelated}\n",
    "# print(lbweight)\n",
    "labelcat = []\n",
    "for lb in label:\n",
    "    labelcat.append(lbtype[lb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.06666666666666667, 1: 0.2, 2: 0.0625}\n"
     ]
    }
   ],
   "source": [
    "print(lbweight)\n",
    "# print(lbweight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_Q_LEN = data1.shape[1]\n",
    "# MAX_A_LEN = data2.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71055\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SZ = len(vocab)\n",
    "print(VOCAB_SZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RNN_HIDDEN = 128\n",
    "EMBED_DIM = 256\n",
    "DENSE_HIDDEN = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_matrix = np.load('wv_matrix'+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Sentence1 (InputLayer)          (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Sentence2 (InputLayer)          (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "WordEmbedding (Embedding)       (None, 20, 256)      18190080    Sentence1[0][0]                  \n",
      "                                                                 Sentence2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "RNN1 (CuDNNLSTM)                (None, 128)          197632      WordEmbedding[0][0]              \n",
      "                                                                 WordEmbedding[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Interaction (Concatenate)       (None, 256)          0           RNN1[0][0]                       \n",
      "                                                                 RNN1[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            771         Interaction[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 18,388,483\n",
      "Trainable params: 18,388,483\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "# core\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Activation, BatchNormalization, Dropout\n",
    "from tensorflow.keras.layers import CuDNNGRU, CuDNNLSTM, GRU, LSTM\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Dot, Bidirectional, TimeDistributed, Lambda, Multiply, Concatenate, Flatten\n",
    "\n",
    "# L_EmbeddingLayer = Embedding(VOCAB_SZ, EMBED_DIM, mask_zero=False, weights=[wv_matrix], trainable=True, name='WordEmbedding')\n",
    "L_EmbeddingLayer = Embedding(VOCAB_SZ, EMBED_DIM, name='WordEmbedding')\n",
    "\n",
    "t_query = Input(shape=(MAX_Q_LEN,), dtype='int32', name='Sentence1')\n",
    "t_enc_Q = L_EmbeddingLayer(t_query)\n",
    "\n",
    "t_answer = Input(shape=(MAX_A_LEN,), dtype='int32', name='Sentence2')\n",
    "t_enc_A = L_EmbeddingLayer(t_answer)\n",
    "\n",
    "RNNLayer1 = CuDNNLSTM(RNN_HIDDEN, unit_forget_bias=True, name='RNN1')\n",
    "# MaxPoolingLayer = GlobalMaxPooling1D(name='Pooling')\n",
    "\n",
    "semi_out_q = RNNLayer1(t_enc_Q)\n",
    "# semi_out_q = RNNLayer3(semi_out_q)\n",
    "# semi_out_q = MaxPoolingLayer(semi_out_q)\n",
    "\n",
    "semi_out_a = RNNLayer1(t_enc_A)\n",
    "# semi_out_a = RNNLayer3(semi_out_a)\n",
    "# semi_out_a = MaxPoolingLayer(semi_out_a)\n",
    "\n",
    "DenseLayer2 = Dense(3, activation='softmax', use_bias=True)\n",
    "# DropLayer1 = Dropout(0.5)\n",
    "# DropLayer2 = Dropout(0.5)\n",
    "\n",
    "semi_out_qa = Concatenate(axis=-1, name='Interaction')([semi_out_q, semi_out_a])\n",
    "# semi_out_qa = Multiply(name='Interaction')([semi_out_q, semi_out_a])\n",
    "# semi_out_qa = DropLayer1(semi_out_qa)\n",
    "# semi_out_qa = DenseLayer1(semi_out_qa)\n",
    "# semi_out_qa = DropLayer2(semi_out_qa)\n",
    "output = DenseLayer2(semi_out_qa)\n",
    "\n",
    "model = Model(inputs=[t_query, t_answer], outputs=output)\n",
    "\n",
    "model.summary()\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model_summary.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/george/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "320000/320552 [============================>.] - ETA: 0s - loss: 0.4373 - acc: 0.8029WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 13s 39us/step - loss: 0.4372 - acc: 0.8030\n",
      "Epoch 2/100\n",
      "320000/320552 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.8675WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 10s 32us/step - loss: 0.3036 - acc: 0.8675\n",
      "Epoch 3/100\n",
      "320000/320552 [============================>.] - ETA: 0s - loss: 0.2503 - acc: 0.8916WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 10s 32us/step - loss: 0.2503 - acc: 0.8916\n",
      "Epoch 4/100\n",
      "320000/320552 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.9068WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 10s 32us/step - loss: 0.2149 - acc: 0.9067\n",
      "Epoch 5/100\n",
      "319488/320552 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9179WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 10s 31us/step - loss: 0.1890 - acc: 0.9179\n",
      "Epoch 6/100\n",
      "320000/320552 [============================>.] - ETA: 0s - loss: 0.1691 - acc: 0.9265WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 10s 32us/step - loss: 0.1692 - acc: 0.9265\n",
      "Epoch 7/100\n",
      "320000/320552 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9334WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 10s 32us/step - loss: 0.1537 - acc: 0.9334\n",
      "Epoch 8/100\n",
      "319488/320552 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9390WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 10s 32us/step - loss: 0.1410 - acc: 0.9389\n",
      "Epoch 9/100\n",
      "319488/320552 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9436WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 10s 32us/step - loss: 0.1305 - acc: 0.9435\n",
      "Epoch 10/100\n",
      "320512/320552 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9476WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 10s 32us/step - loss: 0.1213 - acc: 0.9477\n",
      "Epoch 11/100\n",
      "319488/320552 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9507WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 10s 32us/step - loss: 0.1140 - acc: 0.9507\n",
      "Epoch 12/100\n",
      "319488/320552 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9539WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 10s 32us/step - loss: 0.1070 - acc: 0.9538\n",
      "Epoch 13/100\n",
      "320512/320552 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9559WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 10s 32us/step - loss: 0.1013 - acc: 0.9559\n",
      "Epoch 14/100\n",
      "320512/320552 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9581WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 10s 32us/step - loss: 0.0962 - acc: 0.9581\n",
      "Epoch 15/100\n",
      "320512/320552 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9604WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 10s 32us/step - loss: 0.0918 - acc: 0.9604\n",
      "Epoch 16/100\n",
      "320000/320552 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9616WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 10s 32us/step - loss: 0.0884 - acc: 0.9616\n",
      "Epoch 17/100\n",
      "318976/320552 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9632WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 11s 35us/step - loss: 0.0841 - acc: 0.9632\n",
      "Epoch 18/100\n",
      "318976/320552 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9644WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "320552/320552 [==============================] - 10s 32us/step - loss: 0.0810 - acc: 0.9643\n",
      "Epoch 19/100\n",
      " 39424/320552 [==>...........................] - ETA: 8s - loss: 0.0639 - acc: 0.9732"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c558d3a15c5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m model.fit(x=[data1, data2],y=labelcat, batch_size=512, epochs=epochs, \\\n\u001b[1;32m     18\u001b[0m            \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstarting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m           callbacks=[earlystp, lrreduc, checkpoint])\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LambdaCallback\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "epochs = 100\n",
    "starting = 0\n",
    "optimizer = RMSprop(lr=0.001, clipnorm=15.)\n",
    "# optimizer = Adam(lr=0.001, clipnorm=15.)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "earlystp = EarlyStopping(monitor=\"val_loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "checkpoint = ModelCheckpoint(model_name+'_{epoch:02d}.hdf5', monitor='val_loss', \\\n",
    "                             verbose=0, save_best_only=False, save_weights_only=False, \\\n",
    "                             mode='auto', period=1)\n",
    "lrreduc = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=5, min_lr=0.00001, verbose=1, cooldown=3)\n",
    "# evaluate(None,None)\n",
    "model.fit(x=[data1, data2],y=labelcat, batch_size=512, epochs=epochs, \\\n",
    "           initial_epoch=starting, shuffle=True)\n",
    "#           callbacks=[earlystp, lrreduc, checkpoint])\n",
    "\n",
    "model.save(model_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test.csv ...\n",
      "done. 62767 data loaded.\n"
     ]
    }
   ],
   "source": [
    "##### Test Data Loader\n",
    "print(\"Loading {} ...\".format(test_name))\n",
    "testsents = {}\n",
    "testdata1 = []\n",
    "testdata2 = []\n",
    "testid = []\n",
    "Reader = csv.reader(open(test_name, newline='', encoding='utf-8'), delimiter=',', quotechar='\"')\n",
    "for i,fields in enumerate(Reader):    \n",
    "    if i == 0:\n",
    "        continue\n",
    "    testid.append(fields[0])\n",
    "    tid1, tid2 = fields[1:3]\n",
    "    if mode == 'english':\n",
    "        sent1 = fields[5]\n",
    "        sent2 = fields[6]\n",
    "    elif mode == 'chinese':\n",
    "        sent1 = fields[3]\n",
    "        sent2 = fields[4]\n",
    "    if sent1 == \"\":\n",
    "        sent1 = UNK\n",
    "    if sent2 == \"\":\n",
    "        sent2 = UNK    \n",
    "    if tid1 not in testsents:\n",
    "        testsents[tid1] = sent1 \n",
    "    if tid2 not in testsents:\n",
    "        testsents[tid2] = sent2\n",
    "    testdata1.append(tid1)\n",
    "    testdata2.append(tid2)\n",
    "NUM_DATA = len(testsents)\n",
    "print(\"done. {} data loaded.\".format(NUM_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727658fc34e04f868667f6cf8afb4462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=62767), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.739 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##### Sent2Seq\n",
    "def sent2seq():\n",
    "    for key, sent in tqdm(testsents.items()):\n",
    "        words = pseg.cut(sent)\n",
    "        out_seq = []\n",
    "        for w,flag in words:\n",
    "            if flag is not 'x':\n",
    "                try:\n",
    "                    wid = vocab[w]\n",
    "                except KeyError:\n",
    "                    wid = vocab[UNK]\n",
    "                out_seq.append(wid)\n",
    "        testsents[key] = out_seq\n",
    "\n",
    "    sent2seq_test_name = 'sent2seq_test.json'\n",
    "    json.dump(testsents, open(sent2seq_test_name, 'w'))\n",
    "# sent2seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "testsents = json.load(open(sent2seq_test_name))\n",
    "\n",
    "for i,d in enumerate(testdata1):\n",
    "#     seq = [iBOS] + testsents[d] + [iEOS]\n",
    "    seq = testsents[d]\n",
    "    testdata1[i] = seq\n",
    "for i,d in enumerate(testdata2):\n",
    "#     seq = [iBOS] + testsents[d] + [iEOS]\n",
    "    seq = testsents[d]\n",
    "    testdata2[i] = seq\n",
    "    \n",
    "testdata1 = pad_sequences(testdata1, maxlen=MAX_Q_LEN, padding='pre', truncating='pre', value=iPAD)\n",
    "testdata2 = pad_sequences(testdata2, maxlen=MAX_A_LEN, padding='pre', truncating='pre', value=iPAD)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80126/80126 [==============================] - 0s 6us/step\n"
     ]
    }
   ],
   "source": [
    "testprobs = model.predict(x=[testdata1, testdata2], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlabel = [np.argmax(lb) for lb in testprobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "type2lb = ['agreed', 'disagreed', 'unrelated']\n",
    "outcsv = open('predict.csv', 'w')\n",
    "outcsv.write(\"Id,Category\\n\")\n",
    "for t, lb in zip(testid, testlabel):\n",
    "    outcsv.write(\"{},{}\\n\".format(t, type2lb[lb]))\n",
    "outcsv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
