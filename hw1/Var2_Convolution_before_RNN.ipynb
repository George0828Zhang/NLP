{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup 1\n",
    "- train/test name: the files containing the relationship between titles\n",
    "- model_name: the name of the model. Used when saving model.\n",
    "- sent2seq_name: the output of Preprocessing task.\n",
    "- sent2seq_test_name: the file to store the processed sentences in testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 4\n",
    "train_name = 'train.csv'\n",
    "test_name = 'test.csv'\n",
    "model_name = 'models/RNN(CNN)_basic'\n",
    "mode = 'chinese' # english / chinese\n",
    "vocab_name = 'vocab.json'\n",
    "sent2seq_name = 'sent2seq.json'\n",
    "sent2seq_test_name = 'sent2seq_test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import jieba.posseg as pseg\n",
    "import csv\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup 2\n",
    "- MAX_Q_LEN: the maximum length of the first input title\n",
    "- MAX_A_LEN: the maximum length of the second input title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<bos>'\n",
    "EOS = '<eos>'\n",
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "MAX_Q_LEN = 20\n",
    "MAX_A_LEN = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "this cell loads the sentence relationships, store it into label, and store the corresponding sentences into data1, data2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocab.json\n",
      "done\n",
      "loading sent2seq.json\n",
      "done\n",
      "Loading train.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39427092da834be8ab251940197fd8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=320553), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done. 320552 data loaded.\n"
     ]
    }
   ],
   "source": [
    "##### Data Loader\n",
    "print('loading ' + vocab_name)\n",
    "vocab = json.load(open(vocab_name, 'r', encoding='utf-8'))\n",
    "print('done')\n",
    "\n",
    "print('loading ' + sent2seq_name)\n",
    "Sent2Seq = json.load(open(sent2seq_name, 'r'))\n",
    "print('done')\n",
    "\n",
    "iBOS = vocab[BOS]\n",
    "iPAD = vocab[PAD]\n",
    "iEOS = vocab[EOS]\n",
    "\n",
    "print(\"Loading {} ...\".format(train_name))\n",
    "data1 = []\n",
    "data2 = []\n",
    "label = []\n",
    "\n",
    "Reader = csv.reader(open(train_name, newline='', encoding='utf-8'), delimiter=',', quotechar='\"')\n",
    "row_count = sum(1 for row in Reader)\n",
    "Reader = csv.reader(open(train_name, newline='', encoding='utf-8'), delimiter=',', quotechar='\"')\n",
    "for i,fields in tqdm(enumerate(Reader), total=row_count):    \n",
    "    if i == 0:\n",
    "        continue\n",
    "    tid1, tid2 = fields[1:3]\n",
    "#     sent1 = [iBOS] + Sent2Seq[tid1] + [iEOS]\n",
    "#     sent2 = [iBOS] + Sent2Seq[tid2] + [iEOS]\n",
    "    sent1 = Sent2Seq[tid1]\n",
    "    sent2 = Sent2Seq[tid2]\n",
    "    data1.append(sent1)\n",
    "    data2.append(sent2)\n",
    "    label.append(fields[7])\n",
    "NUM_DATA = len(data1)\n",
    "print(\"done. {} data loaded.\".format(NUM_DATA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding & Label transform\n",
    "the cell pads or truncate the sentences to the specified length, then assign number (0~2) according to the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "data1 = pad_sequences(data1, maxlen=MAX_Q_LEN, padding='pre', truncating='pre', value=iPAD)\n",
    "data2 = pad_sequences(data2, maxlen=MAX_A_LEN, padding='pre', truncating='pre', value=iPAD)\n",
    "print('done')\n",
    "\n",
    "num_agree = sum([lb == 'agreed' for lb in label])\n",
    "num_disagree = sum([lb == 'disagreed' for lb in label])\n",
    "num_unrelated = sum([lb == 'unrelated' for lb in label])\n",
    "\n",
    "lbtype = {'agreed': 0, 'disagreed': 1, 'unrelated': 2}\n",
    "lbweight = {lbtype['agreed']: 1/15, lbtype['disagreed']: 1/5, lbtype['unrelated']: 1/16}\n",
    "# lbweight = {lbtype['agreed']: NUM_DATA/num_agree, lbtype['disagreed']: NUM_DATA/num_disagree, lbtype['unrelated']: NUM_DATA/num_unrelated}\n",
    "# print(lbweight)\n",
    "labelcat = []\n",
    "for lb in label:\n",
    "    labelcat.append(lbtype[lb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "hyperparameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RNN_HIDDEN = 128\n",
    "EMBED_DIM = 64\n",
    "DENSE_HIDDEN = 1\n",
    "CNN_fNUM = 32\n",
    "CNN_fSIZE = 5\n",
    "VOCAB_SZ = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wv_matrix = np.load('wv_matrix'+'.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "this cell specifies the architecture of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/b05902066/miniconda3/envs/joanneflow/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Sentence1 (InputLayer)          (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Sentence2 (InputLayer)          (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "WordEmbedding (Embedding)       (None, 20, 64)       4547520     Sentence1[0][0]                  \n",
      "                                                                 Sentence2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 32)       10272       WordEmbedding[0][0]              \n",
      "                                                                 WordEmbedding[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "RNN1 (CuDNNLSTM)                (None, 128)          82944       conv1d[0][0]                     \n",
      "                                                                 conv1d[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Interaction (Concatenate)       (None, 256)          0           RNN1[0][0]                       \n",
      "                                                                 RNN1[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            771         Interaction[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 4,641,507\n",
      "Trainable params: 4,641,507\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "# core\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Activation, BatchNormalization, Dropout\n",
    "from tensorflow.keras.layers import CuDNNGRU, CuDNNLSTM, GRU, LSTM, Conv1D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Dot, Bidirectional, TimeDistributed, Lambda, Multiply, Concatenate, Flatten\n",
    "\n",
    "\n",
    "# L_EmbeddingLayer = Embedding(VOCAB_SZ, EMBED_DIM, mask_zero=False, weights=[wv_matrix], trainable=True, name='WordEmbedding')\n",
    "L_EmbeddingLayer = Embedding(VOCAB_SZ, EMBED_DIM, name='WordEmbedding')\n",
    "\n",
    "t_query = Input(shape=(MAX_Q_LEN,), dtype='int32', name='Sentence1')\n",
    "t_enc_Q = L_EmbeddingLayer(t_query)\n",
    "\n",
    "t_answer = Input(shape=(MAX_A_LEN,), dtype='int32', name='Sentence2')\n",
    "t_enc_A = L_EmbeddingLayer(t_answer)\n",
    "\n",
    "CNNLayer = Conv1D(CNN_fNUM, CNN_fSIZE, padding='same')\n",
    "RNNLayer1 = CuDNNLSTM(RNN_HIDDEN, unit_forget_bias=True, name='RNN1')\n",
    "MaxPoolingLayer = GlobalMaxPooling1D(name='Pooling')\n",
    "\n",
    "semi_out_q = CNNLayer(t_enc_Q)\n",
    "semi_out_q = RNNLayer1(semi_out_q)\n",
    "\n",
    "semi_out_a = CNNLayer(t_enc_A)\n",
    "semi_out_a = RNNLayer1(semi_out_a)\n",
    "\n",
    "DenseLayer2 = Dense(3, activation='softmax', use_bias=True)\n",
    "\n",
    "semi_out_qa = Concatenate(axis=-1, name='Interaction')([semi_out_q, semi_out_a])\n",
    "\n",
    "output = DenseLayer2(semi_out_qa)\n",
    "\n",
    "model = Model(inputs=[t_query, t_answer], outputs=output)\n",
    "\n",
    "model.summary()\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "# plot_model(model, to_file='model_summary.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "this cell trains the model with the data prepared above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320552/320552 [==============================] - 169s 528us/sample - loss: 0.4095 - acc: 0.8108\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LambdaCallback\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# increasing GPU memory usage automaticaly\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "# config\n",
    "tf.keras.backend.set_session(sess)\n",
    "\n",
    "epochs = 1\n",
    "starting = 0\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x=[data1, data2],y=labelcat, batch_size=16, epochs=epochs, \\\n",
    "           initial_epoch=starting, shuffle=True)\n",
    "\n",
    "model.save(model_name+'_'+str(CNN_fNUM)+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_name+'_'+str(CNN_fNUM)+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_name+'_'+str(CNN_fNUM)+'.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testdata processing\n",
    "the following two cells process the testing data the same way as the training data was processed in Preprocessing and above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test.csv ...\n",
      "done. 62767 data loaded.\n"
     ]
    }
   ],
   "source": [
    "##### Test Data Loader\n",
    "print(\"Loading {} ...\".format(test_name))\n",
    "testsents = {}\n",
    "testdata1 = []\n",
    "testdata2 = []\n",
    "testid = []\n",
    "Reader = csv.reader(open(test_name, newline='', encoding='utf-8'), delimiter=',', quotechar='\"')\n",
    "for i,fields in enumerate(Reader):    \n",
    "    if i == 0:\n",
    "        continue\n",
    "    testid.append(fields[0])\n",
    "    tid1, tid2 = fields[1:3]\n",
    "    if mode == 'english':\n",
    "        sent1 = fields[5]\n",
    "        sent2 = fields[6]\n",
    "    elif mode == 'chinese':\n",
    "        sent1 = fields[3]\n",
    "        sent2 = fields[4]\n",
    "    if sent1 == \"\":\n",
    "        sent1 = UNK\n",
    "    if sent2 == \"\":\n",
    "        sent2 = UNK    \n",
    "    if tid1 not in testsents:\n",
    "        testsents[tid1] = sent1 \n",
    "    if tid2 not in testsents:\n",
    "        testsents[tid2] = sent2\n",
    "    testdata1.append(tid1)\n",
    "    testdata2.append(tid2)\n",
    "NUM_DATA = len(testsents)\n",
    "print(\"done. {} data loaded.\".format(NUM_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Sent2Seq\n",
    "def sent2seq():\n",
    "    for key, sent in tqdm(testsents.items()):\n",
    "        words = pseg.cut(sent)\n",
    "        out_seq = []\n",
    "        for w,flag in words:\n",
    "            if flag is not 'x':\n",
    "                try:\n",
    "                    wid = vocab[w]\n",
    "                except KeyError:\n",
    "                    wid = vocab[UNK]\n",
    "                out_seq.append(wid)\n",
    "        testsents[key] = out_seq\n",
    "\n",
    "    sent2seq_test_name = 'sent2seq_test.json'\n",
    "    json.dump(testsents, open(sent2seq_test_name, 'w'))\n",
    "# sent2seq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testdata padding\n",
    "loads the processed testdata and pad them to the input size of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "testsents = json.load(open(sent2seq_test_name))\n",
    "\n",
    "for i,d in enumerate(testdata1):\n",
    "#     seq = [iBOS] + testsents[d] + [iEOS]\n",
    "    seq = testsents[d]\n",
    "    testdata1[i] = seq\n",
    "for i,d in enumerate(testdata2):\n",
    "#     seq = [iBOS] + testsents[d] + [iEOS]\n",
    "    seq = testsents[d]\n",
    "    testdata2[i] = seq\n",
    "    \n",
    "testdata1 = pad_sequences(testdata1, maxlen=MAX_Q_LEN, padding='pre', truncating='pre', value=iPAD)\n",
    "testdata2 = pad_sequences(testdata2, maxlen=MAX_A_LEN, padding='pre', truncating='pre', value=iPAD)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing\n",
    "inference the model to obtain the probability distribution of each title pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80126/80126 [==============================] - 0s 5us/sample\n"
     ]
    }
   ],
   "source": [
    "testprobs = model.predict(x=[testdata1, testdata2], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine class & output\n",
    "use argmax to determine the most probable relation for each pairs, and then output the test results to predict.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlabel = [np.argmax(lb) for lb in testprobs]\n",
    "type2lb = ['agreed', 'disagreed', 'unrelated']\n",
    "outcsv = open('predict.csv', 'w')\n",
    "outcsv.write(\"Id,Category\\n\")\n",
    "for t, lb in zip(testid, testlabel):\n",
    "    outcsv.write(\"{},{}\\n\".format(t, type2lb[lb]))\n",
    "outcsv.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
