{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 4\n",
    "train_name = 'train.csv'\n",
    "test_name = 'test.csv'\n",
    "model_name = 'models/RNNv1'\n",
    "mode = 'chinese' # english / chinese\n",
    "vocab_name = 'vocab.json'\n",
    "sent2seq_name = 'sent2seq.json'\n",
    "sent2seq_test_name = 'sent2seq_test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import jieba.posseg as pseg\n",
    "import csv\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<bos>'\n",
    "EOS = '<eos>'\n",
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "MAX_Q_LEN = 30\n",
    "MAX_A_LEN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocab.json\n",
      "done\n",
      "loading sent2seq.json\n",
      "done\n",
      "Loading train.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a35fb4dfe76426d805b152c1ae50efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=320553), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done. 320552 data loaded.\n"
     ]
    }
   ],
   "source": [
    "##### Data Loader\n",
    "print('loading ' + vocab_name)\n",
    "vocab = json.load(open(vocab_name, 'r', encoding='utf-8'))\n",
    "print('done')\n",
    "\n",
    "print('loading ' + sent2seq_name)\n",
    "Sent2Seq = json.load(open(sent2seq_name, 'r'))\n",
    "print('done')\n",
    "\n",
    "iBOS = vocab[BOS]\n",
    "iPAD = vocab[PAD]\n",
    "iEOS = vocab[EOS]\n",
    "\n",
    "print(\"Loading {} ...\".format(train_name))\n",
    "data1 = []\n",
    "data2 = []\n",
    "label = []\n",
    "\n",
    "Reader = csv.reader(open(train_name, newline='', encoding='utf-8'), delimiter=',', quotechar='\"')\n",
    "row_count = sum(1 for row in Reader)\n",
    "Reader = csv.reader(open(train_name, newline='', encoding='utf-8'), delimiter=',', quotechar='\"')\n",
    "for i,fields in tqdm(enumerate(Reader), total=row_count):    \n",
    "    if i == 0:\n",
    "        continue\n",
    "    tid1, tid2 = fields[1:3]\n",
    "#     sent1 = [iBOS] + Sent2Seq[tid1] + [iEOS]\n",
    "#     sent2 = [iBOS] + Sent2Seq[tid2] + [iEOS]\n",
    "    sent1 = Sent2Seq[tid1]\n",
    "    sent2 = Sent2Seq[tid2]\n",
    "    data1.append(sent1)\n",
    "    data2.append(sent2)\n",
    "    label.append(fields[7])\n",
    "NUM_DATA = len(data1)\n",
    "print(\"done. {} data loaded.\".format(NUM_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# data1 = pad_sequences(data1, maxlen=MAX_Q_LEN, padding='pre', truncating='pre', value=iPAD)\n",
    "# data2 = pad_sequences(data2, maxlen=MAX_A_LEN, padding='pre', truncating='pre', value=iPAD)\n",
    "data1 = pad_sequences(data1, maxlen=None, padding='pre', truncating='pre', value=iPAD)\n",
    "data2 = pad_sequences(data2, maxlen=None, padding='pre', truncating='pre', value=iPAD)\n",
    "print('done')\n",
    "\n",
    "num_agree = sum([lb == 'agreed' for lb in label])\n",
    "num_disagree = sum([lb == 'disagreed' for lb in label])\n",
    "num_unrelated = sum([lb == 'unrelated' for lb in label])\n",
    "\n",
    "lbtype = {'agreed': 0, 'disagreed': 1, 'unrelated': 2}\n",
    "lbweight = {lbtype['agreed']: 1/15, lbtype['disagreed']: 1/5, lbtype['unrelated']: 1/16}\n",
    "# lbweight = {lbtype['agreed']: NUM_DATA/num_agree, lbtype['disagreed']: NUM_DATA/num_disagree, lbtype['unrelated']: NUM_DATA/num_unrelated}\n",
    "# print(lbweight)\n",
    "labelcat = []\n",
    "for lb in label:\n",
    "    labelcat.append(lbtype[lb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.06666666666666667, 1: 0.2, 2: 0.0625}\n"
     ]
    }
   ],
   "source": [
    "print(lbweight)\n",
    "# print(lbweight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_Q_LEN = data1.shape[1]\n",
    "MAX_A_LEN = data2.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23225\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SZ = len(vocab)\n",
    "print(VOCAB_SZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_matrix = np.load('wv_matrix'+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EMBED_DIM = wv_matrix.shape[1]\n",
    "RNN_HIDDEN = EMBED_DIM + 1\n",
    "DENSE_HIDDEN = EMBED_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Sentence1 (InputLayer)          (None, 74)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Sentence2 (InputLayer)          (None, 81)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "WordEmbedding (Embedding)       multiple             4645000     Sentence1[0][0]                  \n",
      "                                                                 Sentence2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   multiple             648024      WordEmbedding[0][0]              \n",
      "                                                                 WordEmbedding[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 81, 402)      161604      bidirectional[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Attention (Dot)                 (None, 74, 81)       0           bidirectional[0][0]              \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Softmax)               (None, 74, 81)       0           Attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ReweightQ (Dot)                 (None, 81, 402)      0           softmax[0][0]                    \n",
      "                                                                 bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Interaction (Multiply)          (None, 81, 402)      0           ReweightQ[0][0]                  \n",
      "                                                                 bidirectional[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Interaction2 (Subtract)         (None, 81, 402)      0           ReweightQ[0][0]                  \n",
      "                                                                 bidirectional[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Pooling (GlobalMaxPooling1D)    (None, 402)          0           Interaction[0][0]                \n",
      "                                                                 Interaction2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "MultiInclusion (Concatenate)    (None, 804)          0           Pooling[0][0]                    \n",
      "                                                                 Pooling[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200)          161000      MultiInclusion[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            603         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,616,231\n",
      "Trainable params: 5,616,231\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "# core\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Activation, BatchNormalization, Dropout, Softmax, Subtract\n",
    "from tensorflow.keras.layers import CuDNNGRU, CuDNNLSTM, GRU, LSTM\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Dot, Bidirectional, TimeDistributed, Lambda, Multiply, Concatenate, Flatten\n",
    "\n",
    "L_EmbeddingLayer = Embedding(VOCAB_SZ, EMBED_DIM, mask_zero=False, weights=[wv_matrix], trainable=True, name='WordEmbedding')\n",
    "# L_EmbeddingLayer = Embedding(VOCAB_SZ, EMBED_DIM, name='WordEmbedding')\n",
    "\n",
    "t_query = Input(shape=(MAX_Q_LEN,), dtype='int32', name='Sentence1')\n",
    "t_enc_Q = L_EmbeddingLayer(t_query)\n",
    "\n",
    "t_answer = Input(shape=(MAX_A_LEN,), dtype='int32', name='Sentence2')\n",
    "t_enc_A = L_EmbeddingLayer(t_answer)\n",
    "\n",
    "RNNLayer1 = Bidirectional(CuDNNLSTM(RNN_HIDDEN, unit_forget_bias=True, return_sequences=True, name='RNN1'), merge_mode='concat')\n",
    "MaxPoolingLayer = GlobalMaxPooling1D(name='Pooling')\n",
    "ATTNWLayer = Dense(RNN_HIDDEN*2, activation=None, use_bias=False)\n",
    "ATTNLayer = Dot(axes=-1, name=\"Attention\")\n",
    "QReWeightLayer = Dot(axes=-2, name=\"ReweightQ\")\n",
    "DenseLayer1 = Dense(DENSE_HIDDEN, activation='tanh', use_bias=True)\n",
    "DenseLayer2 = Dense(3, activation='softmax', use_bias=True)\n",
    "DropLayer1 = Dropout(0.5)\n",
    "DropLayer2 = Dropout(0.5)\n",
    "\n",
    "h_q = RNNLayer1(t_enc_Q)\n",
    "h_a = RNNLayer1(t_enc_A)\n",
    "\n",
    "Wh_a = ATTNWLayer(h_a)\n",
    "ATTN_mat = ATTNLayer([h_q, Wh_a])\n",
    "ATTN_q2a = Softmax(axis=1)(ATTN_mat)\n",
    "\n",
    "h_hat_q = QReWeightLayer([ATTN_q2a, h_q])\n",
    "\n",
    "\n",
    "inter_mul = Multiply(name='Interaction')([h_hat_q, h_a])\n",
    "inter_sub = Subtract(name='Interaction2')([h_hat_q, h_a])\n",
    "max_filtered_mul = MaxPoolingLayer(inter_mul)\n",
    "max_filtered_sub = MaxPoolingLayer(inter_sub)\n",
    "\n",
    "feature_concat = Concatenate(axis=-1, name='MultiInclusion')([max_filtered_mul, max_filtered_sub])\n",
    "# feature_concat = DropLayer1(feature_concat)\n",
    "semi_out_qa = DenseLayer1(feature_concat)\n",
    "# semi_out_qa = DropLayer2(semi_out_qa)\n",
    "output = DenseLayer2(semi_out_qa)\n",
    "\n",
    "model = Model(inputs=[t_query, t_answer], outputs=output)\n",
    "\n",
    "model.summary()\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model_summary.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# L_EmbeddingLayer = load_model('ElmoLayer.h5')\n",
    "# for layer in L_EmbeddingLayer.layers:\n",
    "#     layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Model, load_model\n",
    "# from tensorflow.keras import backend as K\n",
    "# # core\n",
    "# from tensorflow.keras.layers import Input, Dense, Embedding, Activation, BatchNormalization, Dropout, Softmax, Subtract\n",
    "# from tensorflow.keras.layers import CuDNNGRU, CuDNNLSTM, GRU, LSTM\n",
    "# from tensorflow.keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "# from tensorflow.keras.layers import Dot, Bidirectional, TimeDistributed, Lambda, Multiply, Concatenate, Flatten\n",
    "\n",
    "# t_query = Input(shape=(MAX_Q_LEN,), dtype='int32', name='Sentence1')\n",
    "# t_enc_Q = L_EmbeddingLayer(t_query)\n",
    "\n",
    "# t_answer = Input(shape=(MAX_A_LEN,), dtype='int32', name='Sentence2')\n",
    "# t_enc_A = L_EmbeddingLayer(t_answer)\n",
    "\n",
    "# print(t_enc_Q.shape, t_enc_A.shape)\n",
    "\n",
    "# RNNLayer1 = Bidirectional(CuDNNLSTM(RNN_HIDDEN, unit_forget_bias=True, return_sequences=True, name='RNN1'), merge_mode='concat')\n",
    "# MaxPoolingLayer = GlobalMaxPooling1D(name='Pooling')\n",
    "# ATTNWLayer = Dense(RNN_HIDDEN*2, activation=None, use_bias=False)\n",
    "# ATTNLayer = Dot(axes=-1, name=\"Attention\")\n",
    "# QReWeightLayer = Dot(axes=-2, name=\"ReweightQ\")\n",
    "# DenseLayer1 = Dense(DENSE_HIDDEN, activation='tanh', use_bias=True)\n",
    "# DenseLayer2 = Dense(3, activation='softmax', use_bias=True)\n",
    "# DropLayer1 = Dropout(0.5)\n",
    "# DropLayer2 = Dropout(0.5)\n",
    "\n",
    "# h_q = RNNLayer1(t_enc_Q)\n",
    "# h_a = RNNLayer1(t_enc_A)\n",
    "\n",
    "# Wh_a = ATTNWLayer(h_a)\n",
    "# ATTN_mat = ATTNLayer([h_q, Wh_a])\n",
    "# ATTN_q2a = Softmax(axis=1)(ATTN_mat)\n",
    "\n",
    "# h_hat_q = QReWeightLayer([ATTN_q2a, h_q])\n",
    "\n",
    "\n",
    "# inter_mul = Multiply(name='Interaction')([h_hat_q, h_a])\n",
    "# inter_sub = Subtract(name='Interaction2')([h_hat_q, h_a])\n",
    "# max_filtered_mul = MaxPoolingLayer(inter_mul)\n",
    "# max_filtered_sub = MaxPoolingLayer(inter_sub)\n",
    "\n",
    "# feature_concat = Concatenate(axis=-1, name='MultiInclusion')([max_filtered_mul, max_filtered_sub])\n",
    "# # feature_concat = DropLayer1(feature_concat)\n",
    "# semi_out_qa = DenseLayer1(feature_concat)\n",
    "# # semi_out_qa = DropLayer2(semi_out_qa)\n",
    "# output = DenseLayer2(semi_out_qa)\n",
    "\n",
    "# model = Model(inputs=[t_query, t_answer], outputs=output)\n",
    "\n",
    "# model.summary()\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "# plot_model(model, to_file='model_summary.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/george/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 304524 samples, validate on 16028 samples\n",
      "Epoch 1/30\n",
      "304524/304524 [==============================] - 86s 283us/step - loss: 0.4166 - acc: 0.8059 - val_loss: 0.4513 - val_acc: 0.7831\n",
      "Epoch 2/30\n",
      "304524/304524 [==============================] - 83s 272us/step - loss: 0.2582 - acc: 0.8900 - val_loss: 0.4153 - val_acc: 0.8127\n",
      "Epoch 3/30\n",
      "304524/304524 [==============================] - 83s 274us/step - loss: 0.1918 - acc: 0.9208 - val_loss: 0.4605 - val_acc: 0.8125\n",
      "Epoch 4/30\n",
      "304524/304524 [==============================] - 84s 276us/step - loss: 0.1480 - acc: 0.9398 - val_loss: 0.4953 - val_acc: 0.8168\n",
      "Epoch 5/30\n",
      " 56832/304524 [====>.........................] - ETA: 1:06 - loss: 0.0981 - acc: 0.9629"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-db0764e5a249>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# evaluate(None,None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m model.fit(x=[data1, data2],y=labelcat, batch_size=512, epochs=epochs, \\\n\u001b[0;32m---> 18\u001b[0;31m            initial_epoch=starting, shuffle=True, validation_split=0.05)#, class_weight=lbweight)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#           callbacks=[earlystp, lrreduc, checkpoint])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LambdaCallback\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "epochs = 30\n",
    "starting = 0\n",
    "optimizer = RMSprop(lr=0.001, clipnorm=15.)\n",
    "# optimizer = Adam(lr=0.001, clipnorm=15.)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "earlystp = EarlyStopping(monitor=\"val_loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "checkpoint = ModelCheckpoint(model_name+'_{epoch:02d}.hdf5', monitor='val_loss', \\\n",
    "                             verbose=0, save_best_only=False, save_weights_only=False, \\\n",
    "                             mode='auto', period=1)\n",
    "lrreduc = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=5, min_lr=0.00001, verbose=1, cooldown=3)\n",
    "# evaluate(None,None)\n",
    "model.fit(x=[data1, data2],y=labelcat, batch_size=512, epochs=epochs, \\\n",
    "           initial_epoch=starting, shuffle=True, validation_split=0.05)#, class_weight=lbweight)\n",
    "#           callbacks=[earlystp, lrreduc, checkpoint])\n",
    "\n",
    "model.save(model_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test.csv ...\n",
      "done. 62767 data loaded.\n"
     ]
    }
   ],
   "source": [
    "##### Test Data Loader\n",
    "print(\"Loading {} ...\".format(test_name))\n",
    "testsents = {}\n",
    "testdata1 = []\n",
    "testdata2 = []\n",
    "testid = []\n",
    "Reader = csv.reader(open(test_name, newline='', encoding='utf-8'), delimiter=',', quotechar='\"')\n",
    "for i,fields in enumerate(Reader):    \n",
    "    if i == 0:\n",
    "        continue\n",
    "    testid.append(fields[0])\n",
    "    tid1, tid2 = fields[1:3]\n",
    "    if mode == 'english':\n",
    "        sent1 = fields[5]\n",
    "        sent2 = fields[6]\n",
    "    elif mode == 'chinese':\n",
    "        sent1 = fields[3]\n",
    "        sent2 = fields[4]\n",
    "    if sent1 == \"\":\n",
    "        sent1 = UNK\n",
    "    if sent2 == \"\":\n",
    "        sent2 = UNK    \n",
    "    if tid1 not in testsents:\n",
    "        testsents[tid1] = sent1 \n",
    "    if tid2 not in testsents:\n",
    "        testsents[tid2] = sent2\n",
    "    testdata1.append(tid1)\n",
    "    testdata2.append(tid2)\n",
    "NUM_DATA = len(testsents)\n",
    "print(\"done. {} data loaded.\".format(NUM_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Sent2Seq\n",
    "def sent2seq():\n",
    "    for key, sent in tqdm(testsents.items()):\n",
    "        words = pseg.cut(sent)\n",
    "        out_seq = []\n",
    "        for w,flag in words:\n",
    "            if flag is not 'x':\n",
    "                try:\n",
    "                    wid = vocab[w]\n",
    "                except KeyError:\n",
    "                    wid = vocab[UNK]\n",
    "                out_seq.append(wid)\n",
    "        testsents[key] = out_seq\n",
    "\n",
    "    sent2seq_test_name = 'sent2seq_test.json'\n",
    "    json.dump(testsents, open(sent2seq_test_name, 'w'))\n",
    "# sent2seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "testsents = json.load(open(sent2seq_test_name))\n",
    "\n",
    "for i,d in enumerate(testdata1):\n",
    "#     seq = [iBOS] + testsents[d] + [iEOS]\n",
    "    seq = testsents[d]\n",
    "    testdata1[i] = seq\n",
    "for i,d in enumerate(testdata2):\n",
    "#     seq = [iBOS] + testsents[d] + [iEOS]\n",
    "    seq = testsents[d]\n",
    "    testdata2[i] = seq\n",
    "    \n",
    "testdata1 = pad_sequences(testdata1, maxlen=MAX_Q_LEN, padding='pre', truncating='pre', value=iPAD)\n",
    "testdata2 = pad_sequences(testdata2, maxlen=MAX_A_LEN, padding='pre', truncating='pre', value=iPAD)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80126/80126 [==============================] - 8s 97us/step\n"
     ]
    }
   ],
   "source": [
    "testprobs = model.predict(x=[testdata1, testdata2], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlabel = [np.argmax(lb) for lb in testprobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "type2lb = ['agreed', 'disagreed', 'unrelated']\n",
    "outcsv = open('predict.csv', 'w')\n",
    "outcsv.write(\"Id,Category\\n\")\n",
    "for t, lb in zip(testid, testlabel):\n",
    "    outcsv.write(\"{},{}\\n\".format(t, type2lb[lb]))\n",
    "outcsv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
