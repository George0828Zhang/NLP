{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "vocab_name, sent2seq_name: the files will be used latter by wordembedding and rnn training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 4\n",
    "train_name = 'train.csv'\n",
    "test_name = 'test.csv'\n",
    "mode = 'chinese' # english / chinese\n",
    "vocab_name = 'vocab.json'\n",
    "sent2seq_name = 'sent2seq.json'\n",
    "min_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import jieba.posseg as pseg\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<bos>'\n",
    "EOS = '<eos>'\n",
    "PAD = '<pad>'\n",
    "UNK = '<unk>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "loads the sentences into the dictionary, with their id as key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train.csv ...\n",
      "done. 167564 data loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading {} ...\".format(train_name))\n",
    "sents = {}\n",
    "Reader = csv.reader(open(train_name, newline='', encoding='utf-8'), delimiter=',', quotechar='\"')\n",
    "for i,fields in enumerate(Reader):    \n",
    "    if i == 0:\n",
    "        continue\n",
    "    tid1, tid2 = fields[1:3]\n",
    "    if mode == 'english':\n",
    "        sent1 = fields[5]\n",
    "        sent2 = fields[6]\n",
    "    elif mode == 'chinese':\n",
    "        sent1 = fields[3]\n",
    "        sent2 = fields[4]\n",
    "    if sent1 == \"\":\n",
    "        sent1 = UNK\n",
    "    if sent2 == \"\":\n",
    "        sent2 = UNK\n",
    "    if tid1 not in sents:\n",
    "        sents[tid1] = sent1 \n",
    "    if tid2 not in sents:\n",
    "        sents[tid2] = sent2 \n",
    "NUM_DATA = len(sents)\n",
    "print(\"done. {} data loaded.\".format(NUM_DATA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency counting\n",
    "this cell segments the sentences into words, then accummulates the frequency for all words. This is useful if we want to eliminate low frequency words, when min_count > 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee556177b9c9435d893e60555d7f19a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=167564), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.890 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "71055\n"
     ]
    }
   ],
   "source": [
    "freq = {'<pad>':min_count, '<bos>':min_count, '<eos>':min_count, '<unk>': min_count}\n",
    "\n",
    "for key, sent in tqdm(sents.items()):\n",
    "    words = pseg.cut(sent)\n",
    "    segsent = []\n",
    "    for w,flag in words:\n",
    "        if flag is not 'x':\n",
    "            try:\n",
    "                freq[w] += 1\n",
    "            except KeyError:\n",
    "                freq[w] = 1\n",
    "            segsent.append(w)\n",
    "    sents[key] = segsent\n",
    "print(len(freq))\n",
    "json.dump(freq, open(\"tmp_word_freq.json\", 'w', encoding='utf-8'))\n",
    "json.dump(sents, open(\"tmp_seg_words.json\", 'w', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "this cell uses the frequency counted above to select words to put into the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71055\n"
     ]
    }
   ],
   "source": [
    "freq = json.load(open(\"tmp_word_freq.json\", 'r', encoding='utf-8'))\n",
    "sents = json.load(open(\"tmp_seg_words.json\", 'r', encoding='utf-8'))\n",
    "vocab = {}\n",
    "for w, f in freq.items():\n",
    "    if f >= min_count:\n",
    "        vocab[w] = len(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sent2Seq\n",
    "this cell translates all sentences into sequences of indices of words, then store it and vocabulary into corresponding files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402b16f024384128a672f9e6dec559eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=167564), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dumping data to vocab.json\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def words2seq(pair):\n",
    "    key = pair[0]\n",
    "    words = pair[1]\n",
    "    out_seq = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            wid = vocab[w]\n",
    "        except KeyError:\n",
    "            wid = vocab[UNK]\n",
    "        out_seq.append(wid)\n",
    "    return (key, out_seq)\n",
    "\n",
    "stmp = {}\n",
    "for pair in tqdm(sents.items()):    \n",
    "    key, out_seq = words2seq(pair)\n",
    "    stmp[key] = out_seq\n",
    "sents = stmp\n",
    "    \n",
    "print('dumping data to ' + vocab_name)\n",
    "json.dump(vocab, open(vocab_name, 'w', encoding='utf-8'))\n",
    "json.dump(sents, open(sent2seq_name, 'w'))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean():\n",
    "    !rm tmp_word_freq.json tmp_seg_words.json\n",
    "# clean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
